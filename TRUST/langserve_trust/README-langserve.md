Here’s a sample `README-langserve.md` you can drop into the root of your **TRUST** repo (alongside the new `langserve_trust/` folder). It’s written as a step-by-step developer guide:

---

# TRUST + LangServe Deployment

This document describes how to expose the **TRUST** system through [LangServe](https://python.langchain.com/docs/langserve), providing a REST API and auto-generated docs.

---

## 📂 Directory Layout

Inside the cloned `TRUST/` repo:

```
TRUST/
├─ trust/                # existing TRUST source
├─ configs/              # (existing config files)
├─ langserve_trust/      # NEW: LangServe wrapper
│  ├─ server.py          # FastAPI app entrypoint
│  ├─ client_example.py  # local test client
│  ├─ requirements.txt   # or pyproject.toml
│  └─ trust_adapter/
│     ├─ __init__.py
│     ├─ schemas.py      # Pydantic I/O models
│     └─ runner.py       # TrustRunnable adapter
└─ README-langserve.md   # this file
```

---

## ⚙️ Installation

1. Clone the repo:

   ```bash
   git clone https://github.com/emorynlp/TRUST.git
   cd TRUST/langserve_trust
   ```

2. Create a virtual environment and install requirements:

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

3. Make sure the TRUST package itself is importable:

   ```bash
   pip install -e ..
   ```

---

## ▶️ Running the Server

Start the LangServe app with:

```bash
uvicorn server:app --reload --port 8000
```

* Visit [http://localhost:8000/docs](http://localhost:8000/docs) for the interactive Swagger UI.
* Available endpoints (auto-generated by LangServe):

  * `POST /agent/invoke` — single request/response
  * `POST /agent/batch` — list of inputs in one call
  * `POST /agent/stream` — SSE streaming (if implemented)

A simple healthcheck is available at:

```bash
curl http://localhost:8000/health
```

---

## 💡 Client Usage Example

Use `RemoteRunnable` to call the TRUST agent from Python:

```python
from langserve import RemoteRunnable

model = RemoteRunnable("http://localhost:8000/agent/")

payload = {
    "session_id": "demo-001",
    "message": "Hi, I had a bad dream last night and feel on edge.",
    "history": [{"role": "assistant", "content": "Hello, I'm here to listen."}],
    "config": {"temperature": 0.2}
}

resp = model.invoke(payload)
print(resp)  # {"text": "...", "state": {...}}
```

---

## 🛠 Wiring TRUST into the Adapter

Edit `langserve_trust/trust_adapter/runner.py` in two places:

1. **Loader** (`_load_trust`) → connect to TRUST’s initialization (e.g., config file or checkpoint).
2. **Inference** (`invoke`) → call the TRUST response method (`respond(...)` / `engine.step(...)`) and return a `TrustOutput`.

---

## 🚀 Deployment Notes

* For production, consider:

  * `uvicorn server:app --host 0.0.0.0 --port 8000 --workers 4`
  * Adding a `Dockerfile`
* Environment variables (e.g., model paths, keys) can be managed in a `.env` file and loaded inside `server.py`.

---

✅ Once wired, TRUST is available as a REST API that plugs into any LangChain pipeline or external service via LangServe.

---

Do you want me to also draft a **Dockerfile** for this LangServe wrapper so you could deploy TRUST in a container (local/remote/Cloud Run/Heroku)?
