Hereâ€™s a sample `README-langserve.md` you can drop into the root of your **TRUST** repo (alongside the new `langserve_trust/` folder). Itâ€™s written as a step-by-step developer guide:

---

# TRUST + LangServe Deployment

This document describes how to expose the **TRUST** system through [LangServe](https://python.langchain.com/docs/langserve), providing a REST API and auto-generated docs.

---

## ğŸ“‚ Directory Layout

Inside the cloned `TRUST/` repo:

```
TRUST/
â”œâ”€ trust/                # existing TRUST source
â”œâ”€ configs/              # (existing config files)
â”œâ”€ langserve_trust/      # NEW: LangServe wrapper
â”‚  â”œâ”€ server.py          # FastAPI app entrypoint
â”‚  â”œâ”€ client_example.py  # local test client
â”‚  â”œâ”€ requirements.txt   # or pyproject.toml
â”‚  â””â”€ trust_adapter/
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ schemas.py      # Pydantic I/O models
â”‚     â””â”€ runner.py       # TrustRunnable adapter
â””â”€ README-langserve.md   # this file
```

---

## âš™ï¸ Installation

1. Clone the repo:

   ```bash
   git clone https://github.com/emorynlp/TRUST.git
   cd TRUST/langserve_trust
   ```

2. Create a virtual environment and install requirements:

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

3. Make sure the TRUST package itself is importable:

   ```bash
   pip install -e ..
   ```

---

## â–¶ï¸ Running the Server

Start the LangServe app with:

```bash
uvicorn server:app --reload --port 8000
```

* Visit [http://localhost:8000/docs](http://localhost:8000/docs) for the interactive Swagger UI.
* Available endpoints (auto-generated by LangServe):

  * `POST /agent/invoke` â€” single request/response
  * `POST /agent/batch` â€” list of inputs in one call
  * `POST /agent/stream` â€” SSE streaming (if implemented)

A simple healthcheck is available at:

```bash
curl http://localhost:8000/health
```

---

## ğŸ’¡ Client Usage Example

Use `RemoteRunnable` to call the TRUST agent from Python:

```python
from langserve import RemoteRunnable

model = RemoteRunnable("http://localhost:8000/agent/")

payload = {
    "session_id": "demo-001",
    "message": "Hi, I had a bad dream last night and feel on edge.",
    "history": [{"role": "assistant", "content": "Hello, I'm here to listen."}],
    "config": {"temperature": 0.2}
}

resp = model.invoke(payload)
print(resp)  # {"text": "...", "state": {...}}
```

---

## ğŸ›  Wiring TRUST into the Adapter

Edit `langserve_trust/trust_adapter/runner.py` in two places:

1. **Loader** (`_load_trust`) â†’ connect to TRUSTâ€™s initialization (e.g., config file or checkpoint).
2. **Inference** (`invoke`) â†’ call the TRUST response method (`respond(...)` / `engine.step(...)`) and return a `TrustOutput`.

---

## ğŸš€ Deployment Notes

* For production, consider:

  * `uvicorn server:app --host 0.0.0.0 --port 8000 --workers 4`
  * Adding a `Dockerfile`
* Environment variables (e.g., model paths, keys) can be managed in a `.env` file and loaded inside `server.py`.

---

âœ… Once wired, TRUST is available as a REST API that plugs into any LangChain pipeline or external service via LangServe.

---

Do you want me to also draft a **Dockerfile** for this LangServe wrapper so you could deploy TRUST in a container (local/remote/Cloud Run/Heroku)?
